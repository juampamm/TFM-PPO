{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(x, scores, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states),\\\n",
    "            np.array(self.actions),\\\n",
    "            np.array(self.probs),\\\n",
    "            np.array(self.vals),\\\n",
    "            np.array(self.rewards),\\\n",
    "            np.array(self.dones),\\\n",
    "            batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class ActorNetwork(keras.Model):\n",
    "    def __init__(self, n_actions, fc1_dims=256, fc2_dims=256):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.fc1 = Dense(fc1_dims, activation='relu')\n",
    "        self.fc2 = Dense(fc2_dims, activation='relu')\n",
    "        self.fc3 = Dense(n_actions, activation='softmax')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CriticNetwork(keras.Model):\n",
    "    def __init__(self, fc1_dims=256, fc2_dims=256):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1 = Dense(fc1_dims, activation='relu')\n",
    "        self.fc2 = Dense(fc2_dims, activation='relu')\n",
    "        self.q = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        q = self.q(x)\n",
    "\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.0003,\n",
    "                 gae_lambda=0.95, policy_clip=0.2, batch_size=64,\n",
    "                 n_epochs=10, chkpt_dir='models/'):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.chkpt_dir = chkpt_dir\n",
    "\n",
    "        self.actor = ActorNetwork(n_actions)\n",
    "        self.actor.compile(optimizer=Adam(learning_rate=alpha))\n",
    "        self.critic = CriticNetwork()\n",
    "        self.critic.compile(optimizer=Adam(learning_rate=alpha))\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "\n",
    "    def store_transition(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        print('... saving models ...')\n",
    "        self.actor.save(self.chkpt_dir + 'actor')\n",
    "        self.critic.save(self.chkpt_dir + 'critic')\n",
    "\n",
    "    def load_models(self):\n",
    "        print('... loading models ...')\n",
    "        self.actor = keras.models.load_model(self.chkpt_dir + 'actor')\n",
    "        self.critic = keras.models.load_model(self.chkpt_dir + 'critic')\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = tf.convert_to_tensor([observation])\n",
    "\n",
    "        probs = self.actor(state)\n",
    "        dist = tfp.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        value = self.critic(state)\n",
    "\n",
    "        action = action.numpy()[0]\n",
    "        value = value.numpy()[0]\n",
    "        log_prob = log_prob.numpy()[0]\n",
    "\n",
    "        return action, log_prob, value\n",
    "\n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
    "                reward_arr, dones_arr, batches = \\\n",
    "                self.memory.generate_batches()\n",
    "\n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1] * (\n",
    "                        1-int(dones_arr[k])) - values[k])\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[t] = a_t\n",
    "\n",
    "            for batch in batches:\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    states = tf.convert_to_tensor(state_arr[batch])\n",
    "                    old_probs = tf.convert_to_tensor(old_prob_arr[batch])\n",
    "                    actions = tf.convert_to_tensor(action_arr[batch])\n",
    "\n",
    "                    probs = self.actor(states)\n",
    "                    dist = tfp.distributions.Categorical(probs)\n",
    "                    new_probs = dist.log_prob(actions)\n",
    "\n",
    "                    critic_value = self.critic(states)\n",
    "\n",
    "                    critic_value = tf.squeeze(critic_value, 1)\n",
    "\n",
    "                    prob_ratio = tf.math.exp(new_probs - old_probs)\n",
    "                    weighted_probs = advantage[batch] * prob_ratio\n",
    "                    clipped_probs = tf.clip_by_value(prob_ratio,\n",
    "                                                     1-self.policy_clip,\n",
    "                                                     1+self.policy_clip)\n",
    "                    weighted_clipped_probs = clipped_probs * advantage[batch]\n",
    "                    actor_loss = -tf.math.minimum(weighted_probs,\n",
    "                                                  weighted_clipped_probs)\n",
    "                    actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "                    returns = advantage[batch] + values[batch]\n",
    "                    # critic_loss = tf.math.reduce_mean(tf.math.pow(\n",
    "                    #                                  returns-critic_value, 2))\n",
    "                    critic_loss = keras.losses.MSE(critic_value, returns)\n",
    "\n",
    "                actor_params = self.actor.trainable_variables\n",
    "                actor_grads = tape.gradient(actor_loss, actor_params)\n",
    "                critic_params = self.critic.trainable_variables\n",
    "                critic_grads = tape.gradient(critic_loss, critic_params)\n",
    "                self.actor.optimizer.apply_gradients(\n",
    "                        zip(actor_grads, actor_params))\n",
    "                self.critic.optimizer.apply_gradients(\n",
    "                        zip(critic_grads, critic_params))\n",
    "\n",
    "        self.memory.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 score 22.0 avg score 22.0 time_steps 22 learning_steps 1\n",
      "episode 1 score 47.0 avg score 34.5 time_steps 69 learning_steps 3\n",
      "episode 2 score 13.0 avg score 27.3 time_steps 82 learning_steps 4\n",
      "episode 3 score 19.0 avg score 25.2 time_steps 101 learning_steps 5\n",
      "episode 4 score 43.0 avg score 28.8 time_steps 144 learning_steps 7\n",
      "episode 5 score 18.0 avg score 27.0 time_steps 162 learning_steps 8\n",
      "episode 6 score 58.0 avg score 31.4 time_steps 220 learning_steps 11\n",
      "episode 7 score 14.0 avg score 29.2 time_steps 234 learning_steps 11\n",
      "episode 8 score 14.0 avg score 27.6 time_steps 248 learning_steps 12\n",
      "episode 9 score 24.0 avg score 27.2 time_steps 272 learning_steps 13\n",
      "episode 10 score 9.0 avg score 25.5 time_steps 281 learning_steps 14\n",
      "episode 11 score 36.0 avg score 26.4 time_steps 317 learning_steps 15\n",
      "episode 12 score 14.0 avg score 25.5 time_steps 331 learning_steps 16\n",
      "episode 13 score 20.0 avg score 25.1 time_steps 351 learning_steps 17\n",
      "episode 14 score 13.0 avg score 24.3 time_steps 364 learning_steps 18\n",
      "episode 15 score 15.0 avg score 23.7 time_steps 379 learning_steps 18\n",
      "episode 16 score 52.0 avg score 25.4 time_steps 431 learning_steps 21\n",
      "episode 17 score 14.0 avg score 24.7 time_steps 445 learning_steps 22\n",
      "episode 18 score 16.0 avg score 24.3 time_steps 461 learning_steps 23\n",
      "episode 19 score 16.0 avg score 23.9 time_steps 477 learning_steps 23\n",
      "episode 20 score 11.0 avg score 23.2 time_steps 488 learning_steps 24\n",
      "episode 21 score 15.0 avg score 22.9 time_steps 503 learning_steps 25\n",
      "Steps:  512\n",
      "episode 22 score 38.0 avg score 23.5 time_steps 541 learning_steps 27\n",
      "episode 23 score 11.0 avg score 23.0 time_steps 552 learning_steps 27\n",
      "episode 24 score 48.0 avg score 24.0 time_steps 600 learning_steps 30\n",
      "episode 25 score 32.0 avg score 24.3 time_steps 632 learning_steps 31\n",
      "episode 26 score 24.0 avg score 24.3 time_steps 656 learning_steps 32\n",
      "episode 27 score 23.0 avg score 24.2 time_steps 679 learning_steps 33\n",
      "episode 28 score 38.0 avg score 24.7 time_steps 717 learning_steps 35\n",
      "episode 29 score 46.0 avg score 25.4 time_steps 763 learning_steps 38\n",
      "episode 30 score 133.0 avg score 28.9 time_steps 896 learning_steps 44\n",
      "episode 31 score 37.0 avg score 29.2 time_steps 933 learning_steps 46\n",
      "episode 32 score 33.0 avg score 29.3 time_steps 966 learning_steps 48\n",
      "episode 33 score 20.0 avg score 29.0 time_steps 986 learning_steps 49\n",
      "Steps:  1024\n",
      "episode 34 score 56.0 avg score 29.8 time_steps 1042 learning_steps 52\n",
      "episode 35 score 161.0 avg score 33.4 time_steps 1203 learning_steps 60\n",
      "episode 36 score 20.0 avg score 33.1 time_steps 1223 learning_steps 61\n",
      "episode 37 score 66.0 avg score 33.9 time_steps 1289 learning_steps 64\n",
      "episode 38 score 38.0 avg score 34.0 time_steps 1327 learning_steps 66\n",
      "episode 39 score 12.0 avg score 33.5 time_steps 1339 learning_steps 66\n",
      "episode 40 score 55.0 avg score 34.0 time_steps 1394 learning_steps 69\n",
      "episode 41 score 81.0 avg score 35.1 time_steps 1475 learning_steps 73\n",
      "episode 42 score 18.0 avg score 34.7 time_steps 1493 learning_steps 74\n",
      "Steps:  1536\n",
      "episode 43 score 77.0 avg score 35.7 time_steps 1570 learning_steps 78\n",
      "episode 44 score 129.0 avg score 37.8 time_steps 1699 learning_steps 84\n",
      "episode 45 score 15.0 avg score 37.3 time_steps 1714 learning_steps 85\n",
      "episode 46 score 44.0 avg score 37.4 time_steps 1758 learning_steps 87\n",
      "episode 47 score 23.0 avg score 37.1 time_steps 1781 learning_steps 89\n",
      "episode 48 score 20.0 avg score 36.8 time_steps 1801 learning_steps 90\n",
      "episode 49 score 61.0 avg score 37.2 time_steps 1862 learning_steps 93\n",
      "episode 50 score 23.0 avg score 37.0 time_steps 1885 learning_steps 94\n",
      "episode 51 score 46.0 avg score 37.1 time_steps 1931 learning_steps 96\n",
      "episode 52 score 104.0 avg score 38.4 time_steps 2035 learning_steps 101\n",
      "Steps:  2048\n",
      "episode 53 score 96.0 avg score 39.5 time_steps 2131 learning_steps 106\n",
      "episode 54 score 195.0 avg score 42.3 time_steps 2326 learning_steps 116\n",
      "episode 55 score 83.0 avg score 43.0 time_steps 2409 learning_steps 120\n",
      "episode 56 score 24.0 avg score 42.7 time_steps 2433 learning_steps 121\n",
      "Steps:  2560\n",
      "episode 57 score 200.0 avg score 45.4 time_steps 2633 learning_steps 131\n",
      "episode 58 score 138.0 avg score 47.0 time_steps 2771 learning_steps 138\n",
      "episode 59 score 17.0 avg score 46.5 time_steps 2788 learning_steps 139\n",
      "episode 60 score 130.0 avg score 47.8 time_steps 2918 learning_steps 145\n",
      "episode 61 score 87.0 avg score 48.5 time_steps 3005 learning_steps 150\n",
      "Steps:  3072\n",
      "episode 62 score 110.0 avg score 49.4 time_steps 3115 learning_steps 155\n",
      "episode 63 score 10.0 avg score 48.8 time_steps 3125 learning_steps 156\n",
      "episode 64 score 38.0 avg score 48.7 time_steps 3163 learning_steps 158\n",
      "episode 65 score 90.0 avg score 49.3 time_steps 3253 learning_steps 162\n",
      "episode 66 score 66.0 avg score 49.5 time_steps 3319 learning_steps 165\n",
      "episode 67 score 21.0 avg score 49.1 time_steps 3340 learning_steps 167\n",
      "episode 68 score 104.0 avg score 49.9 time_steps 3444 learning_steps 172\n",
      "episode 69 score 33.0 avg score 49.7 time_steps 3477 learning_steps 173\n",
      "episode 70 score 93.0 avg score 50.3 time_steps 3570 learning_steps 178\n",
      "Steps:  3584\n",
      "episode 71 score 14.0 avg score 49.8 time_steps 3584 learning_steps 179\n",
      "episode 72 score 193.0 avg score 51.7 time_steps 3777 learning_steps 188\n",
      "episode 73 score 134.0 avg score 52.9 time_steps 3911 learning_steps 195\n",
      "Steps:  4096\n",
      "episode 74 score 200.0 avg score 54.8 time_steps 4111 learning_steps 205\n",
      "episode 75 score 62.0 avg score 54.9 time_steps 4173 learning_steps 208\n",
      "episode 76 score 48.0 avg score 54.8 time_steps 4221 learning_steps 211\n",
      "episode 77 score 153.0 avg score 56.1 time_steps 4374 learning_steps 218\n",
      "episode 78 score 17.0 avg score 55.6 time_steps 4391 learning_steps 219\n",
      "episode 79 score 46.0 avg score 55.5 time_steps 4437 learning_steps 221\n",
      "episode 80 score 126.0 avg score 56.3 time_steps 4563 learning_steps 228\n",
      "Steps:  4608\n",
      "episode 81 score 154.0 avg score 57.5 time_steps 4717 learning_steps 235\n",
      "episode 82 score 67.0 avg score 57.6 time_steps 4784 learning_steps 239\n",
      "episode 83 score 200.0 avg score 59.3 time_steps 4984 learning_steps 249\n",
      "episode 84 score 118.0 avg score 60.0 time_steps 5102 learning_steps 255\n",
      "episode 85 score 13.0 avg score 59.5 time_steps 5115 learning_steps 255\n",
      "Steps:  5120\n",
      "episode 86 score 132.0 avg score 60.3 time_steps 5247 learning_steps 262\n",
      "episode 87 score 200.0 avg score 61.9 time_steps 5447 learning_steps 272\n",
      "Steps:  5632\n",
      "episode 88 score 193.0 avg score 63.4 time_steps 5640 learning_steps 282\n",
      "episode 89 score 200.0 avg score 64.9 time_steps 5840 learning_steps 292\n",
      "episode 90 score 135.0 avg score 65.7 time_steps 5975 learning_steps 298\n",
      "episode 91 score 78.0 avg score 65.8 time_steps 6053 learning_steps 302\n",
      "episode 92 score 52.0 avg score 65.6 time_steps 6105 learning_steps 305\n",
      "Steps:  6144\n",
      "episode 93 score 189.0 avg score 67.0 time_steps 6294 learning_steps 314\n",
      "episode 94 score 200.0 avg score 68.4 time_steps 6494 learning_steps 324\n",
      "episode 95 score 11.0 avg score 67.8 time_steps 6505 learning_steps 325\n",
      "Steps:  6656\n",
      "episode 96 score 200.0 avg score 69.1 time_steps 6705 learning_steps 335\n",
      "episode 97 score 21.0 avg score 68.6 time_steps 6726 learning_steps 336\n",
      "episode 98 score 200.0 avg score 70.0 time_steps 6926 learning_steps 346\n",
      "episode 99 score 200.0 avg score 71.3 time_steps 7126 learning_steps 356\n",
      "episode 100 score 11.0 avg score 71.2 time_steps 7137 learning_steps 356\n",
      "Steps:  7168\n",
      "episode 101 score 34.0 avg score 71.0 time_steps 7171 learning_steps 358\n",
      "episode 102 score 42.0 avg score 71.3 time_steps 7213 learning_steps 360\n",
      "episode 103 score 200.0 avg score 73.1 time_steps 7413 learning_steps 370\n",
      "episode 104 score 174.0 avg score 74.4 time_steps 7587 learning_steps 379\n",
      "Steps:  7680\n",
      "episode 105 score 114.0 avg score 75.4 time_steps 7701 learning_steps 385\n",
      "episode 106 score 186.0 avg score 76.7 time_steps 7887 learning_steps 394\n",
      "episode 107 score 82.0 avg score 77.3 time_steps 7969 learning_steps 398\n",
      "episode 108 score 88.0 avg score 78.1 time_steps 8057 learning_steps 402\n",
      "Steps:  8192\n",
      "episode 109 score 200.0 avg score 79.8 time_steps 8257 learning_steps 412\n",
      "episode 110 score 122.0 avg score 81.0 time_steps 8379 learning_steps 418\n",
      "episode 111 score 113.0 avg score 81.8 time_steps 8492 learning_steps 424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 112 score 133.0 avg score 82.9 time_steps 8625 learning_steps 431\n",
      "episode 113 score 33.0 avg score 83.1 time_steps 8658 learning_steps 432\n",
      "Steps:  8704\n",
      "episode 114 score 83.0 avg score 83.8 time_steps 8741 learning_steps 437\n",
      "episode 115 score 64.0 avg score 84.3 time_steps 8805 learning_steps 440\n",
      "episode 116 score 181.0 avg score 85.5 time_steps 8986 learning_steps 449\n",
      "episode 117 score 62.0 avg score 86.0 time_steps 9048 learning_steps 452\n",
      "episode 118 score 59.0 avg score 86.5 time_steps 9107 learning_steps 455\n",
      "Steps:  9216\n",
      "episode 119 score 187.0 avg score 88.2 time_steps 9294 learning_steps 464\n",
      "episode 120 score 200.0 avg score 90.1 time_steps 9494 learning_steps 474\n",
      "episode 121 score 200.0 avg score 91.9 time_steps 9694 learning_steps 484\n",
      "episode 122 score 16.0 avg score 91.7 time_steps 9710 learning_steps 485\n",
      "Steps:  9728\n",
      "episode 123 score 84.0 avg score 92.4 time_steps 9794 learning_steps 489\n",
      "episode 124 score 76.0 avg score 92.7 time_steps 9870 learning_steps 493\n",
      "episode 125 score 144.0 avg score 93.8 time_steps 10014 learning_steps 500\n",
      "episode 126 score 90.0 avg score 94.5 time_steps 10104 learning_steps 505\n",
      "episode 127 score 106.0 avg score 95.3 time_steps 10210 learning_steps 510\n",
      "episode 128 score 25.0 avg score 95.2 time_steps 10235 learning_steps 511\n",
      "Steps:  10240\n",
      "episode 129 score 200.0 avg score 96.7 time_steps 10435 learning_steps 521\n",
      "episode 130 score 39.0 avg score 95.8 time_steps 10474 learning_steps 523\n",
      "episode 131 score 200.0 avg score 97.4 time_steps 10674 learning_steps 533\n",
      "Steps:  10752\n",
      "episode 132 score 118.0 avg score 98.3 time_steps 10792 learning_steps 539\n",
      "episode 133 score 109.0 avg score 99.2 time_steps 10901 learning_steps 545\n",
      "episode 134 score 200.0 avg score 100.6 time_steps 11101 learning_steps 555\n",
      "episode 135 score 55.0 avg score 99.5 time_steps 11156 learning_steps 557\n",
      "episode 136 score 17.0 avg score 99.5 time_steps 11173 learning_steps 558\n",
      "Steps:  11264\n",
      "episode 137 score 200.0 avg score 100.8 time_steps 11373 learning_steps 568\n",
      "episode 138 score 131.0 avg score 101.8 time_steps 11504 learning_steps 575\n",
      "episode 139 score 48.0 avg score 102.1 time_steps 11552 learning_steps 577\n",
      "episode 140 score 16.0 avg score 101.7 time_steps 11568 learning_steps 578\n",
      "episode 141 score 194.0 avg score 102.9 time_steps 11762 learning_steps 588\n",
      "episode 142 score 13.0 avg score 102.8 time_steps 11775 learning_steps 588\n",
      "Steps:  11776\n",
      "episode 143 score 123.0 avg score 103.3 time_steps 11898 learning_steps 594\n",
      "episode 144 score 50.0 avg score 102.5 time_steps 11948 learning_steps 597\n",
      "episode 145 score 111.0 avg score 103.5 time_steps 12059 learning_steps 602\n",
      "episode 146 score 32.0 avg score 103.3 time_steps 12091 learning_steps 604\n",
      "episode 147 score 151.0 avg score 104.6 time_steps 12242 learning_steps 612\n",
      "episode 148 score 37.0 avg score 104.8 time_steps 12279 learning_steps 613\n",
      "Steps:  12288\n",
      "episode 149 score 92.0 avg score 105.1 time_steps 12371 learning_steps 618\n",
      "episode 150 score 22.0 avg score 105.1 time_steps 12393 learning_steps 619\n",
      "episode 151 score 90.0 avg score 105.5 time_steps 12483 learning_steps 624\n",
      "episode 152 score 73.0 avg score 105.2 time_steps 12556 learning_steps 627\n",
      "episode 153 score 145.0 avg score 105.7 time_steps 12701 learning_steps 635\n",
      "episode 154 score 28.0 avg score 104.0 time_steps 12729 learning_steps 636\n",
      "Steps:  12800\n",
      "Time:  2.505163546403249\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    counter_file = 3\n",
    "    NAME = 'GTTF_cartpole_steps_' + str(counter_file) + '.txt'\n",
    "    finish = False\n",
    "    env = gym.make('CartPole-v0')\n",
    "    N = 20\n",
    "    batch_size = 5\n",
    "    rew_steps = []\n",
    "    n_epochs = 4\n",
    "    alpha = 0.0003\n",
    "    agent = Agent(n_actions=env.action_space.n, batch_size=batch_size,\n",
    "                  alpha=alpha, n_epochs=n_epochs,\n",
    "                  input_dims=env.observation_space.shape)\n",
    "    n_games = 300\n",
    "\n",
    "    figure_file = 'plots/cartpole.png'\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for i in range(n_games):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "            agent.store_transition(observation, action,\n",
    "                                   prob, val, reward, done)\n",
    "            \n",
    "            if n_steps > 25*512:\n",
    "                \n",
    "                finish = True\n",
    "                break\n",
    "            \n",
    "            if n_steps % N == 0:\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "\n",
    "            if n_steps % 512 == 0:\n",
    "                print('Steps: ', n_steps)\n",
    "                #rew_steps.append(test(agent))\n",
    "\n",
    "            observation = observation_\n",
    "        \n",
    "        if finish == True:\n",
    "            break\n",
    "\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            #agent.save_models()\n",
    "\n",
    "        print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
    "              'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "    print('Time: ', (time.time()-start)/60)\n",
    "    output_file = open(NAME, 'w')\n",
    "    for rew in rew_steps:\n",
    "        output_file.write(str(rew) + '\\n')\n",
    "\n",
    "    output_file.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
