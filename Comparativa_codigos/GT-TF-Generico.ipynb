{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7715766708056132\n",
      "0.7440051754315694\n",
      "0.7442135492960612\n",
      "0.7399461309115092\n",
      "0.7412630319595337\n",
      "0.7426882783571879\n",
      "0.7460388580958048\n",
      "0.7503896315892538\n",
      "0.7407379388809204\n",
      "0.7576242327690125\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#tf.config.experimental_run_functions_eagerly(True)\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_probability as tfp\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "EPISODES = 100 # Number of times the enviroment is ran\n",
    "LOSS_CLIPPING = 0.2 # Approximated values stated in the original paper\n",
    "\n",
    "LR = 0.0003 # Learning rate \n",
    "NUM_ACTIONS = 4 # Number of possible action in the environment\n",
    "NUM_STATES = 8 # Number of possible states in the exvironment\n",
    "EPOCHS = 10 # Epochs to train the network (recommended between 3 and 30)\n",
    "BATCH_SIZE = 64 # Batch size for the neural nets\n",
    "BUFFER_SIZE = 256 # Buffer of experiences\n",
    "OPTIMIZER = Adam # Optimizer for both actor and critic\n",
    "GAMMA = 0.99 # Used for the estimated reward\n",
    "LAMBDA = 0.95 # Used in the original paper un the GAE\n",
    "\n",
    "\n",
    "\n",
    "class ActorNetwork(keras.Model):\n",
    "    def __init__(self, n_actions, fc1_dims=32, fc2_dims=32):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.fc1 = Dense(fc1_dims, activation='relu', kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        self.fc2 = Dense(fc2_dims, activation='relu', kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        self.fc3 = Dense(n_actions, activation='softmax')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CriticNetwork(keras.Model):\n",
    "    def __init__(self, fc1_dims=32, fc2_dims=32):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1 = Dense(fc1_dims, activation='relu', kernel_initializer='he_uniform')\n",
    "        self.fc2 = Dense(fc2_dims, activation='relu', kernel_initializer='he_uniform')\n",
    "        self.q = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        q = self.q(x)\n",
    "\n",
    "        return q\n",
    "\n",
    "# Combine both Actor and Critic to create the agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, env_name):\n",
    "        # Environment parameters\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        self.episode = 0 # used to track current number episoded since start\n",
    "        self.max_average = 0 # record max average reached\n",
    "        \n",
    "        # Used to plot a grapgh of the train process\n",
    "        self.scores_, self.average_ = [], []\n",
    "\n",
    "        # Create Actor-Critic network models\n",
    "        self.actor = ActorNetwork(NUM_ACTIONS)\n",
    "        self.actor.compile(optimizer=Adam(learning_rate=LR))\n",
    "        self.critic = CriticNetwork()\n",
    "        self.critic.compile(optimizer=Adam(learning_rate=LR))\n",
    "\n",
    "    def test(self): # train every self.Training_batch episodes\n",
    "        \n",
    "        state_arr = np.random.rand(BUFFER_SIZE,NUM_STATES)\n",
    "        old_prob_arr = np.random.rand(BUFFER_SIZE,)\n",
    "        action_arr = np.random.rand(BUFFER_SIZE,)\n",
    "        values = np.random.rand(BUFFER_SIZE,1)\n",
    "        reward_arr = np.random.rand(BUFFER_SIZE,)\n",
    "        advantage = np.random.rand(BUFFER_SIZE,)\n",
    "\n",
    "        \n",
    "        for _ in range(10):\n",
    "            start = time.time()\n",
    "            for ii in range(EPISODES):\n",
    "                for _ in range(EPOCHS):\n",
    "                    batch_start = np.arange(0, BUFFER_SIZE, BATCH_SIZE)\n",
    "                    indices = np.arange(BUFFER_SIZE, dtype=np.int64)\n",
    "                    np.random.shuffle(indices)\n",
    "                    batches = [indices[i:i+BATCH_SIZE] for i in batch_start]       \n",
    "\n",
    "                    for batch in batches:\n",
    "                        with tf.GradientTape(persistent=True) as tape:\n",
    "                            states = tf.convert_to_tensor(state_arr[batch])\n",
    "                            old_probs = tf.convert_to_tensor(old_prob_arr[batch])\n",
    "                            actions = tf.convert_to_tensor(action_arr[batch])\n",
    "\n",
    "                            probs = self.actor(states)\n",
    "                            dist = tfp.distributions.Categorical(probs)\n",
    "                            new_probs = dist.log_prob(actions)\n",
    "\n",
    "                            critic_value = self.critic(states)\n",
    "                            critic_value = tf.squeeze(critic_value, 1)\n",
    "\n",
    "                            prob_ratio = tf.exp(new_probs - old_probs)\n",
    "                            weighted_probs = advantage[batch] * prob_ratio\n",
    "                            clipped_probs = tf.clip_by_value(prob_ratio, 1-LOSS_CLIPPING, 1+LOSS_CLIPPING)\n",
    "                            weighted_clipped_probs = clipped_probs * advantage[batch]\n",
    "                            actor_loss = -tf.math.minimum(weighted_probs, weighted_clipped_probs)\n",
    "                            actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "                            returns = advantage[batch] + values[batch]\n",
    "\n",
    "                            critic_loss = keras.losses.MSE(critic_value, returns)\n",
    "\n",
    "                        actor_params = self.actor.trainable_variables\n",
    "                        actor_grads = tape.gradient(actor_loss, actor_params)\n",
    "                        critic_params = self.critic.trainable_variables\n",
    "                        critic_grads = tape.gradient(critic_loss, critic_params)\n",
    "                        self.actor.optimizer.apply_gradients(zip(actor_grads, actor_params))\n",
    "                        self.critic.optimizer.apply_gradients(zip(critic_grads, critic_params))\n",
    "\n",
    "            print((time.time() - start)/60) \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    env_name = 'LunarLander-v2'\n",
    "    agent = PPOAgent(env_name)\n",
    "    agent.test() # train as PPO, train every batch, trains better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
