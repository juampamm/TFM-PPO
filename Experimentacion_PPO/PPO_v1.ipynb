{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkhTJ57PCnUp"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3CV8b--JCmmo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from humanfriendly import format_timespan\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGrdFHoFClAY"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Hg7--D0MCgrW"
   },
   "outputs": [],
   "source": [
    "ENV = 'CartPole-v0'\n",
    "\n",
    "EPISODES = 5000 # Number of times the enviroment is ran\n",
    "\n",
    "LOSS_CLIPPING = 0.2 # Approximated values stated in the original paper\n",
    "EPOCHS = 10 # Epochs to train the network (recommended between 3 and 30)\n",
    "NOISE = 1.0 # Standard deviation\n",
    "\n",
    "GAMMA = 0.99 # Used for the estimated reward\n",
    "\n",
    "BUFFER_SIZE = 512 # Buffer of experiences\n",
    "BATCH_SIZE = 64 # Batch size for the neural nets\n",
    "NUM_ACTIONS = 2 # Number of possible action in the environment\n",
    "NUM_STATES = 4 # Number of possible states in the exvironment\n",
    "ENTROPY_LOSS = 5e-3 # Constant value for the entropy (entropy used for exploration)\n",
    "LR = 1e-4  # Learning rate\n",
    "\n",
    "# Used to feed the actor when predicting\n",
    "DUMMY_ACTION, DUMMY_VALUE = np.zeros((1, NUM_ACTIONS)), np.zeros((1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4JwA0KcC0to"
   },
   "source": [
    "## Class memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CtyN6YoPDA-D"
   },
   "outputs": [],
   "source": [
    "# Memory of the experiences\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.episode_batch = [[], [], []]\n",
    "        self.global_batch = [[], [], [], []]\n",
    "        self.episode_rewards = []\n",
    "        self.historical_rewards = []\n",
    "\n",
    "    def reset_espisode_batch(self):\n",
    "        self.global_batch = [[], [], [], []]\n",
    "        self.episode_batch = [[], [], []]\n",
    "        self.episode_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paYFSy5-DMj2"
   },
   "source": [
    "## Custom loss function for PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tbVV_h04DVQo"
   },
   "outputs": [],
   "source": [
    "# Custom loss functions for the PPO\n",
    "def proximal_policy_optimization_loss(advantage, old_prediction):\n",
    "    def loss(y_true, y_pred):\n",
    "\n",
    "        \n",
    "        prob = K.sum(y_true * y_pred, axis=-1)\n",
    "        old_prob = K.sum(y_true * old_prediction, axis=-1)\n",
    "        r = prob/(old_prob + 1e-10)\n",
    "        return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantage) + ENTROPY_LOSS * -(prob * K.log(prob + 1e-10)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZZoOZlQDMxI"
   },
   "source": [
    "## PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "25zHyDklDeSu"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    # Constructor of the class\n",
    "    def __init__(self):\n",
    "        self.actor = self.create_actor()\n",
    "        self.critic = self.create_critic()\n",
    "        self.memory = Memory()\n",
    "\n",
    "    # Create the actor used to select the action given an state\n",
    "    def create_actor(self):\n",
    "        # Define three inputs as the advantage and old prediction is used for the custom loss\n",
    "        input_state = Input(shape=(NUM_STATES,))\n",
    "        input_advantage = Input(shape=(1,))\n",
    "        input_old_prediction = Input(shape=(NUM_ACTIONS,))\n",
    "        input_actions = Input(shape=(NUM_ACTIONS,))\n",
    "\n",
    "        layer1 = Dense(128, activation='relu')(input_state)\n",
    "        layer2 = Dense(128, activation='relu')(layer1)\n",
    "\n",
    "        # Softmax as there are different probabilities depending on the action\n",
    "        output_layer = Dense(NUM_ACTIONS, activation='softmax', name='output')(layer2)\n",
    "\n",
    "        model = Model(inputs=[input_state, input_advantage, input_old_prediction, input_actions], outputs=[output_layer])\n",
    "\n",
    "        # Compile the model with the custom loss\n",
    "        model.compile(optimizer=Adam(lr=LR),\n",
    "                      loss=[proximal_policy_optimization_loss(\n",
    "                          advantage=input_advantage,\n",
    "                          old_prediction=input_old_prediction,\n",
    "                          actions=input_actions)])\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Create the critic which will criticise how the actor is performing\n",
    "    def create_critic(self):\n",
    "        # Define the architectire of the network\n",
    "        input_layer = Input(shape=(NUM_STATES,))\n",
    "        layer1 = Dense(128, activation='relu')(input_layer)\n",
    "        layer2 = Dense(128, activation='relu')(layer1)\n",
    "\n",
    "        # Linear output to know how good the action is\n",
    "        ouput_layer = Dense(1)(layer2)\n",
    "\n",
    "        model = Model(inputs=[input_layer], outputs=[ouput_layer])\n",
    "\n",
    "        # Compile it with mse loss and gradient descent\n",
    "        model.compile(optimizer=Adam(lr=LR), \n",
    "                      loss='mse')\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Get the action given the current state\n",
    "    def get_action(self, state):\n",
    "        probabilities = self.actor.predict([state.reshape(1, NUM_STATES), DUMMY_VALUE, DUMMY_ACTION])\n",
    "        action = np.random.choice(NUM_ACTIONS, p=np.nan_to_num(probabilities[0]))\n",
    "        action_matrix = np.zeros(NUM_ACTIONS)\n",
    "        action_matrix[action] = 1\n",
    "        return action, action_matrix, probabilities\n",
    "\n",
    "    # Transform rewards of the episode as the discount has to be applied in the first ones regarding the last ones\n",
    "    def transform_reward(self):\n",
    "        self.memory.historical_rewards.append(sum(self.memory.episode_rewards))\n",
    "\n",
    "        for j in range(len(self.memory.episode_rewards) - 2, -1, -1):\n",
    "            self.memory.episode_rewards[j] += self.memory.episode_rewards[j + 1] * GAMMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9cn63a3FKZH"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1hCNU0d5FN-5"
   },
   "outputs": [],
   "source": [
    "# Create simulated environment\n",
    "def create_environment():\n",
    "    environment = gym.make(ENV)\n",
    "    return environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8SmL-omqFX3Z",
    "outputId": "dde17be7-e7cf-4281-9dad-3a7e4776461a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          640         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          16512       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            258         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 17,410\n",
      "Trainable params: 17,410\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "EPISODE:  1  - SCORE:  30.0  - MEAN SCORE:  30.0\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "EPISODE:  2  - SCORE:  15.0  - MEAN SCORE:  22.5\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "EPISODE:  3  - SCORE:  65.0  - MEAN SCORE:  36.666666666666664\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "EPISODE:  4  - SCORE:  30.0  - MEAN SCORE:  35.0\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "EPISODE:  5  - SCORE:  49.0  - MEAN SCORE:  37.8\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "EPISODE:  6  - SCORE:  76.0  - MEAN SCORE:  44.166666666666664\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "EPISODE:  7  - SCORE:  14.0  - MEAN SCORE:  39.857142857142854\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "EPISODE:  8  - SCORE:  33.0  - MEAN SCORE:  39.0\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "EPISODE:  9  - SCORE:  12.0  - MEAN SCORE:  36.0\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "EPISODE:  10  - SCORE:  34.0  - MEAN SCORE:  35.8\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "EPISODE:  11  - SCORE:  15.0  - MEAN SCORE:  33.90909090909091\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "EPISODE:  12  - SCORE:  14.0  - MEAN SCORE:  32.25\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "EPISODE:  13  - SCORE:  18.0  - MEAN SCORE:  31.153846153846153\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "EPISODE:  14  - SCORE:  16.0  - MEAN SCORE:  30.071428571428573\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "EPISODE:  15  - SCORE:  15.0  - MEAN SCORE:  29.066666666666666\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "EPISODE:  16  - SCORE:  14.0  - MEAN SCORE:  28.125\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "EPISODE:  17  - SCORE:  12.0  - MEAN SCORE:  27.176470588235293\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "EPISODE:  18  - SCORE:  21.0  - MEAN SCORE:  26.833333333333332\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "EPISODE:  19  - SCORE:  15.0  - MEAN SCORE:  26.210526315789473\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "EPISODE:  20  - SCORE:  14.0  - MEAN SCORE:  25.6\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "EPISODE:  21  - SCORE:  29.0  - MEAN SCORE:  25.761904761904763\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "EPISODE:  22  - SCORE:  24.0  - MEAN SCORE:  25.681818181818183\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "EPISODE:  23  - SCORE:  17.0  - MEAN SCORE:  25.304347826086957\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "EPISODE:  24  - SCORE:  20.0  - MEAN SCORE:  25.083333333333332\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "EPISODE:  25  - SCORE:  17.0  - MEAN SCORE:  24.76\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "EPISODE:  26  - SCORE:  15.0  - MEAN SCORE:  24.384615384615383\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "EPISODE:  27  - SCORE:  30.0  - MEAN SCORE:  24.59259259259259\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "EPISODE:  28  - SCORE:  15.0  - MEAN SCORE:  24.25\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "EPISODE:  29  - SCORE:  55.0  - MEAN SCORE:  25.310344827586206\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "EPISODE:  30  - SCORE:  59.0  - MEAN SCORE:  26.433333333333334\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "EPISODE:  31  - SCORE:  19.0  - MEAN SCORE:  26.193548387096776\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "EPISODE:  32  - SCORE:  40.0  - MEAN SCORE:  26.625\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "EPISODE:  33  - SCORE:  27.0  - MEAN SCORE:  26.636363636363637\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "EPISODE:  34  - SCORE:  24.0  - MEAN SCORE:  26.558823529411764\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "EPISODE:  35  - SCORE:  14.0  - MEAN SCORE:  26.2\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "EPISODE:  36  - SCORE:  16.0  - MEAN SCORE:  25.916666666666668\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "EPISODE:  37  - SCORE:  17.0  - MEAN SCORE:  25.675675675675677\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "EPISODE:  38  - SCORE:  50.0  - MEAN SCORE:  26.31578947368421\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "EPISODE:  39  - SCORE:  17.0  - MEAN SCORE:  26.076923076923077\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "EPISODE:  40  - SCORE:  13.0  - MEAN SCORE:  25.75\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "EPISODE:  41  - SCORE:  12.0  - MEAN SCORE:  25.414634146341463\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "EPISODE:  42  - SCORE:  23.0  - MEAN SCORE:  25.357142857142858\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "EPISODE:  43  - SCORE:  13.0  - MEAN SCORE:  25.069767441860463\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[1. 0.]\n",
      "[0. 1.]\n",
      "[0. 1.]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-234efb5a3df1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mlast_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;31m# Add to the episode batch the informetions gathered\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[1;31m# newlines imply flush in subprocesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    398\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[0;32m    399\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    env = create_environment() \n",
    "    ag = Agent()\n",
    "    episode = 1\n",
    "    goal_reached = False\n",
    "\n",
    "    # Iterate until the number of episodes is reached\n",
    "    while episode < EPISODES:\n",
    "        # Reset episode and global batch and the environment to start a new one\n",
    "        ag.memory.reset_espisode_batch()\n",
    "        state = env.reset()\n",
    "\n",
    "        # Iterate until the global batch is bigger than the buffer\n",
    "        while len(ag.memory.global_batch[0]) < BUFFER_SIZE:\n",
    "            # Get the action given the current state\n",
    "            action, action_matrix, predicted_action = ag.get_action(state)\n",
    "            # Step in the enviroment and gather all parameters\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            ag.memory.episode_rewards.append(reward)\n",
    "            last_reward = sum(ag.memory.episode_rewards)\n",
    "            # Add to the episode batch the informetions gathered\n",
    "            ag.memory.episode_batch[0].append(state)\n",
    "            ag.memory.episode_batch[1].append(action_matrix)\n",
    "            ag.memory.episode_batch[2].append(predicted_action)\n",
    "            state = next_state\n",
    "\n",
    "            # If the episode has finished\n",
    "            if done:\n",
    "                # Transform rewards with discount rate\n",
    "                ag.transform_reward()\n",
    "\n",
    "                # Iterate through all instances of the episode batch\n",
    "                for i in range(len(ag.memory.episode_batch[0])):\n",
    "                    obs, action, pred = ag.memory.episode_batch[0][i], ag.memory.episode_batch[1][i], ag.memory.episode_batch[2][i]\n",
    "                    r = ag.memory.episode_rewards[i]\n",
    "\n",
    "                    # Add all experienes to global batch\n",
    "                    ag.memory.global_batch[0].append(obs)\n",
    "                    ag.memory.global_batch[1].append(action)\n",
    "                    ag.memory.global_batch[2].append(pred)\n",
    "                    ag.memory.global_batch[3].append(r)\n",
    "                \n",
    "                # Reset the episode batch, episode reward and environment\n",
    "                ag.memory.episode_batch = [[], [], []]\n",
    "                ag.memory.episode_rewards = []\n",
    "                state = env.reset()\n",
    "\n",
    "                if episode >= 100:\n",
    "                    print('EPISODE: ', episode, ' - SCORE: ', ag.memory.historical_rewards[-1], ' - MEAN SCORE: ', sum(ag.memory.historical_rewards[-100:])/100)\n",
    "\n",
    "                    if sum(ag.memory.historical_rewards[-100:])/100 >= 195:\n",
    "                        goal_reached = True\n",
    "                        break\n",
    "                else:\n",
    "                    print('EPISODE: ', episode, ' - SCORE: ', ag.memory.historical_rewards[-1], ' - MEAN SCORE: ', sum(ag.memory.historical_rewards)/len(ag.memory.historical_rewards))\n",
    "\n",
    "                episode +=1\n",
    "\n",
    "        if goal_reached == True:\n",
    "            break\n",
    "\n",
    "        # Get the arrays to feed te neural nets\n",
    "        obs, action, pred, reward = np.array(ag.memory.global_batch[0]), np.array(ag.memory.global_batch[1]), np.array(ag.memory.global_batch[2]), np.reshape(np.array(ag.memory.global_batch[3]), (len(ag.memory.global_batch[3]), 1))\n",
    "        pred = np.reshape(pred, (pred.shape[0], pred.shape[2]))\n",
    "\n",
    "        # Ensure that they all have the same size\n",
    "        obs, action, pred, reward = obs[:BUFFER_SIZE], action[:BUFFER_SIZE], pred[:BUFFER_SIZE], reward[:BUFFER_SIZE]\n",
    "        old_prediction = pred\n",
    "        pred_values = ag.critic.predict(obs)\n",
    "\n",
    "        # Get advantage with the rewards taken from the environment predictions of the critic given the states\n",
    "        advantage = reward - pred_values\n",
    "\n",
    "        # Update weights of the actor with the states, advanges obtained from the critic and predictions made. Target values are actions\n",
    "        actor_loss = ag.actor.fit([obs, advantage, old_prediction], [action], batch_size=BATCH_SIZE, shuffle=True, epochs=EPOCHS, verbose=0)\n",
    "        # Update weights of the critic with the states. Targe values are actual rewards\n",
    "        critic_loss = ag.critic.fit([obs], [reward], batch_size=BATCH_SIZE, shuffle=True, epochs=EPOCHS, verbose=0)\n",
    "\n",
    "    # Plot a graph with the rewards over the episodes\n",
    "    plt.plot(ag.memory.historical_rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Score\")   \n",
    "    end = time.time()\n",
    "    difference = end - start\n",
    "    print(format_timespan(difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 minutes and 31.55 seconds\n"
     ]
    }
   ],
   "source": [
    "print(format_timespan(difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8ZC0F5eJGyEb"
   },
   "outputs": [],
   "source": [
    "# No necesario para cartpole\n",
    "#!pip install gym[Box_2D]\n",
    "#!pip install box2d-py\n",
    "#!pip install pyglet\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PPO_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
