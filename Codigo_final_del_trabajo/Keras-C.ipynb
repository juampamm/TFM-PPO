{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "import copy\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "EPISODES = 10000  # Number of times the enviroment is ran\n",
    "LOSS_CLIPPING = 0.2  # Approximated values stated in the original paper\n",
    "ENTROPY_LOSS = 0  # Epochs to train the network (recommended between 3 and 30)\n",
    "LR = 0.02  # Learning rate\n",
    "NUM_ACTIONS = 2  # Number of possible action in the environment\n",
    "NUM_STATES = 4  # Number of possible states in the exvironment\n",
    "EPOCHS = 10  # Epochs to train the network (recommended between 3 and 30)\n",
    "BATCH_SIZE = 64  # Batch size for the neural nets\n",
    "BUFFER_SIZE = 2048  # Buffer of experiences\n",
    "SHUFFLE = True  # Whether to shuffle data or not while training\n",
    "OPTIMIZER = Adam  # Optimizer for both actor and critic\n",
    "GAMMA = 0.99  # Used for the estimated reward\n",
    "LAMBDA = 0.95  # Used in the original paper un the GAE\n",
    "NORMALIZE = True  # Whether to normalize GAE or not\n",
    "\n",
    "\n",
    "# Create the actor used to select the action given an state\n",
    "class Actor_Model:\n",
    "    def __init__(self):\n",
    "        X_input = Input(NUM_STATES)\n",
    "\n",
    "        X = Dense(64, activation=\"relu\",\n",
    "                  kernel_initializer='he_uniform')(X_input)\n",
    "        X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "        # Softmax as there are different probabilities depending on the action\n",
    "        output = Dense(NUM_ACTIONS, activation=\"softmax\")(X)\n",
    "\n",
    "        # Compile the model with the custom loss\n",
    "        self.model = Model(inputs=X_input, outputs=output)\n",
    "        self.model.compile(loss=self.ppo_loss, optimizer=OPTIMIZER(lr=LR))\n",
    "\n",
    "    # Custom loss functions for the PPO\n",
    "    def ppo_loss(self, y_true, y_pred):\n",
    "        # Unpack the elements given in the true label\n",
    "        advantages, true_label, actions = y_true[:, :1], y_true[:,\n",
    "                                                                1:1 + NUM_ACTIONS], y_true[:, 1 + NUM_ACTIONS:]\n",
    "\n",
    "        prob = actions * y_pred\n",
    "        old_prob = actions * true_label\n",
    "\n",
    "        ratio = K.exp(K.log(prob + 1e-10) - K.log(old_prob + 1e-10))\n",
    "\n",
    "        p1 = ratio * advantages\n",
    "        p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING,\n",
    "                    max_value=1 + LOSS_CLIPPING) * advantages\n",
    "\n",
    "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "        entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
    "        entropy = ENTROPY_LOSS * K.mean(entropy)\n",
    "\n",
    "        total_loss = actor_loss - entropy\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "# Create the critic which will criticise how the actor is performing\n",
    "class Critic_Model:\n",
    "    def __init__(self):\n",
    "        X_input = Input(NUM_STATES)\n",
    "\n",
    "        X = Dense(64, activation=\"relu\",\n",
    "                  kernel_initializer='he_uniform')(X_input)\n",
    "        X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "        # Linear output to know how good the action is\n",
    "        value = Dense(1)(X)\n",
    "\n",
    "        # Compile it with mse loss and gradient descent\n",
    "        self.model = Model(inputs=X_input, outputs=value)\n",
    "        self.model.compile(loss='mse', optimizer=OPTIMIZER(lr=LR))\n",
    "\n",
    "\n",
    "# Combine both Actor and Critic to create the agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, env_name):\n",
    "        # Environment parameters\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.episode = 0  # used to track current number episoded since start\n",
    "        self.max_average = 0  # record max average reached\n",
    "\n",
    "        # Used to plot a grapgh of the train process\n",
    "        self.scores_, self.average_ = [], []\n",
    "\n",
    "        # Create Actor-Critic network models\n",
    "        self.Actor = Actor_Model()\n",
    "        self.Critic = Critic_Model()\n",
    "\n",
    "        # Names for the models\n",
    "        self.Actor_name = f\"{self.env_name}_PPO_Actor.h5\"\n",
    "        self.Critic_name = f\"{self.env_name}_PPO_Critic.h5\"\n",
    "\n",
    "    # Get the action given the current state\n",
    "    def act(self, state):\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        prediction = self.Actor.model.predict(state)[0]\n",
    "\n",
    "        # Probability based to choose the action\n",
    "        action = np.random.choice(NUM_ACTIONS, p=prediction)\n",
    "        action_onehot = np.zeros([NUM_ACTIONS])\n",
    "        action_onehot[action] = 1\n",
    "        return action, action_onehot, prediction\n",
    "\n",
    "    # Generalized Advantage Estimation implemented in the original paper\n",
    "    def get_gaes(self, rewards, dones, values, next_values):\n",
    "        # Dones are used to track when is the final step of an episode, so next values are no applied\n",
    "        deltas = [r + GAMMA * (1 - d) * nv - v for r, d,\n",
    "                  nv, v in zip(rewards, dones, next_values, values)]\n",
    "        # Convert list to array as .mean() and .std() are used later\n",
    "        deltas = np.stack(deltas)\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "\n",
    "        for t in reversed(range(len(deltas) - 1)):\n",
    "            gaes[t] = gaes[t] + (1 - dones[t]) * GAMMA * LAMBDA * gaes[t + 1]\n",
    "\n",
    "        target = gaes + values\n",
    "        if NORMALIZE:\n",
    "            gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "        return np.vstack(gaes), np.vstack(target)\n",
    "\n",
    "    def learn(self, states, actions, rewards, predictions, dones, next_states):\n",
    "        # Reshape memory to appropriate shape for training\n",
    "        states = np.vstack(states)\n",
    "        next_states = np.vstack(next_states)\n",
    "        actions = np.vstack(actions)\n",
    "        predictions = np.vstack(predictions)\n",
    "\n",
    "        # Get Critic network predictions for state and next state\n",
    "        values = self.Critic.model.predict(states)\n",
    "        next_values = self.Critic.model.predict(next_states)\n",
    "\n",
    "        # Get the advantage\n",
    "        advantages, target = self.get_gaes(\n",
    "            rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
    "\n",
    "        # Stack info to unpack it in the custom loss\n",
    "        y_true_compressed = np.hstack([advantages, predictions, actions])\n",
    "\n",
    "        # Training Actor and Critic networks\n",
    "        a_loss = self.Actor.model.fit(\n",
    "            states, y_true_compressed, epochs=EPOCHS, verbose=0, shuffle=SHUFFLE, batch_size=BATCH_SIZE)\n",
    "        c_loss = self.Critic.model.fit(\n",
    "            states, target, epochs=EPOCHS, verbose=0, shuffle=SHUFFLE, batch_size=BATCH_SIZE)\n",
    "\n",
    "    def load(self):\n",
    "        self.Actor.Actor.load_weights(self.Actor_name)\n",
    "        self.Critic.Critic.load_weights(self.Critic_name)\n",
    "\n",
    "    def save(self):\n",
    "        self.Actor.model.save_weights(self.Actor_name)\n",
    "        self.Critic.model.save_weights(self.Critic_name)\n",
    "\n",
    "    def run(self):  # train every self.Training_batch episodes\n",
    "        start = time.time()\n",
    "        global LR\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, NUM_STATES])\n",
    "        done, score = False, 0\n",
    "        finished = False\n",
    "        while not finished:\n",
    "            # Instantiate or reset games memory\n",
    "            states, next_states, actions, rewards, predictions, dones = [], [], [], [], [], []\n",
    "            for t in range(BUFFER_SIZE):\n",
    "                # Actor picks an action\n",
    "                action, action_onehot, prediction = self.act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # Memorize (state, action, reward) for training\n",
    "                states.append(state)\n",
    "                next_states.append(np.reshape(next_state, [1, NUM_STATES]))\n",
    "                actions.append(action_onehot)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                predictions.append(prediction)\n",
    "                # Update current state\n",
    "                state = np.reshape(next_state, [1, NUM_STATES])\n",
    "                score += reward\n",
    "\n",
    "                if done:\n",
    "                    self.episode += 1\n",
    "                    self.scores_.append(score)\n",
    "\n",
    "                    if self.episode >= 100:\n",
    "                        average = sum(self.scores_[-100:]) / 100\n",
    "                        print('Episode: {:>5}\\t\\tscore: {:>7.2f}\\t\\taverage: {:>7.2f}'.format(self.episode, score,\n",
    "                                                                                              average))\n",
    "                        if average > self.max_average:\n",
    "                            self.max_average = average\n",
    "\n",
    "                        if average > 195:\n",
    "                            plt.plot(self.scores_)\n",
    "                            plt.xlabel(\"Episode\")\n",
    "                            plt.ylabel(\"Score\")\n",
    "                            finished = True\n",
    "                            break\n",
    "\n",
    "                    else:\n",
    "                        print('Episode: {:>5}\\t\\tscore: {:>7.2f}\\t\\taverage: {:>7.2f}'.format(self.episode, score,\n",
    "                                                                                              sum(self.scores_) /\n",
    "                                                                                              len(self.scores_)))\n",
    "\n",
    "                    state, done, score = self.env.reset(), False, 0\n",
    "                    state = np.reshape(state, [1, NUM_STATES])\n",
    "\n",
    "            self.learn(states, actions, rewards,\n",
    "                        predictions, dones, next_states)\n",
    "            if self.episode >= EPISODES:\n",
    "                break\n",
    "        self.env.close()\n",
    "        print((time.time() - start) / 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = 'CartPole-v0'\n",
    "    agent = PPOAgent(env_name)\n",
    "    agent.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
